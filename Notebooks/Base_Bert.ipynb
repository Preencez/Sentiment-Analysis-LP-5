{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis With Hugging Face\n",
        "\n",
        "Hugging Face is an open-source platform that offers machine learning technologies, including pre-built models for various tasks. With their package, you can easily access these models to use them directly or fine-tune them on your own dataset. The platform also allows you to host your trained models, enabling you to utilize them on different devices and applications.\n",
        "\n",
        "To access the full features of the Hugging Face platform, please visit their website and sign in.\n",
        "\n",
        "Text classification with Hugging Face is a powerful capability provided by their models. By leveraging deep learning techniques, these models can analyze and classify text based on its sentiment, among other factors. However, training such models requires substantial computational power, particularly GPU resources. To tackle this, you can use platforms like Colab, GPU cloud providers, or a local machine equipped with an NVIDIA GPU to ensure efficient training and fine-tuning processes.\n",
        "\n",
        "Exploring sentiment analysis with Hugging Face can greatly enhance your natural language processing projects. Visit their website to learn more about the available models and get started with this powerful tool."
      ],
      "metadata": {
        "id": "ipu6feN1Ragh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "JclaVO8xRdIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bd1e70-2f6a-4a79-df69-8f1cacc7909b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub transformers datasets gradio pipreqs"
      ],
      "metadata": {
        "id": "pszEiXWMRhpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "id": "9zFggb1MeCUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "ykJD97cZhoEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "#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"
      ],
      "metadata": {
        "id": "G6u5Oso4grz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    IntervalStrategy,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline,\n",
        "    Trainer\n",
        ")\n"
      ],
      "metadata": {
        "id": "W7FlVzpORzrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nh2fvln4SZGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up my enviroment"
      ],
      "metadata": {
        "id": "tonzGRprSfT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabe W&B\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "DP1JpZg_SeVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "\n",
        "url = \"https://github.com/Azubi-Africa/Career_Accelerator_P5-NLP/raw/master/zindi_challenge/data/Train.csv\"\n",
        "\n",
        "train= pd.read_csv(url)"
      ],
      "metadata": {
        "id": "6obsEDXASxUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "LMdphsjHS6yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "mu0LVoEqUKfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the label and agreement columns have missing datasets"
      ],
      "metadata": {
        "id": "k2Q_XUWYUUv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the row with missing column\n",
        "train[train.isna().any(axis=1)]\n"
      ],
      "metadata": {
        "id": "J1slxP6wVHmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_text = train.iloc[4798]['safe_text']\n",
        "complete_text = train['safe_text'].iloc[4798]\n",
        "complete_text"
      ],
      "metadata": {
        "id": "lcI4O54bVa8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select row by index and assign values to columns\n",
        "train.loc[4798, 'label'] = 0\n",
        "train.loc[4798, 'agreement'] = 0.666667\n",
        "\n",
        "# Use .iloc[] and .iat[] to select and update safe_text column\n",
        "train.iloc[4798, train.columns.get_loc('safe_text')] = complete_text"
      ],
      "metadata": {
        "id": "7fiP36ETUKQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[4798]"
      ],
      "metadata": {
        "id": "ui3TqSowUKNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "rand_tweet_id = str(uuid.uuid4())\n"
      ],
      "metadata": {
        "id": "8b7DNy1iUKH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_index = 4799\n",
        "train.loc[row_index, 'tweet_id'] = rand_tweet_id\n",
        "train.loc[row_index, 'label'] = 1\n",
        "train.loc[row_index, 'agreement'] = 0.666667\n"
      ],
      "metadata": {
        "id": "pD9jOqqpW90t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[row_index, train.columns.get_loc('safe_text')] = train.iloc[row_index, train.columns.get_loc('safe_text')]\n"
      ],
      "metadata": {
        "id": "mLgSeJ57XAV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[4799]"
      ],
      "metadata": {
        "id": "H_aDys-dXGPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.duplicated().sum()"
      ],
      "metadata": {
        "id": "J8eQ7wIVXOHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting of dataset"
      ],
      "metadata": {
        "id": "t3oz-G6yXVH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train data => {train, eval}\n",
        "train, eval = train_test_split(train, test_size=0.2, random_state=42, stratify=train['label'])"
      ],
      "metadata": {
        "id": "REaURc_GXW53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "hhCSNgolXqAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval.head()"
      ],
      "metadata": {
        "id": "NvjFhDvuXp6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"new dataframe shapes: train is {train.shape}, eval is {eval.shape}\")\n"
      ],
      "metadata": {
        "id": "wJP0i31_Xp3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "directory = '/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Save the dataframes as CSV files in the specified directory\n",
        "train.to_csv(os.path.join(directory, \"train_subset.csv\"), index=False)\n",
        "eval.to_csv(os.path.join(directory, \"eval_subset.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "2ZGvhrzsXp0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files={\n",
        "    'train': os.path.join(directory, 'train_subset.csv'),\n",
        "    'eval': os.path.join(directory, 'eval_subset.csv')\n",
        "}, encoding='ISO-8859-1')\n",
        "\n"
      ],
      "metadata": {
        "id": "o1ee9EbIXy_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "3pBBzsDfV0Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to transform the label values\n",
        "def transform_labels(label):\n",
        "    # Extract the label value\n",
        "    label = label['label']\n",
        "    # Map the label value to an integer value\n",
        "    num = 0\n",
        "    if label == -1: #'Negative'\n",
        "        num = 0\n",
        "    elif label == 0: #'Neutral'\n",
        "        num = 1\n",
        "    elif label == 1: #'Positive'\n",
        "        num = 2\n",
        "    # Return a dictionary with a single key-value pair\n",
        "    return {'labels': num}\n",
        "\n",
        "# Define a function to tokenize the text data\n",
        "def tokenize_data(example):\n",
        "    # Extract the 'safe_text' value from the input example and tokenize it\n",
        "    return tokenizer(example['safe_text'], padding='max_length')\n",
        "\n",
        "# Apply the transformation functions to the dataset using the 'map' method\n",
        "# This transforms the label values and tokenizes the text data\n",
        "dataset_out = dataset.map(transform_labels)\n",
        "\n",
        "dataset_base = dataset_out.map(tokenize_data, batched=True)\n",
        "\n",
        "# Define a list of column names to remove from the dataset\n",
        "remove_columns = ['tweet_id', 'label', 'safe_text', 'agreement']\n",
        "\n",
        "# Apply the 'transform_labels' function to the dataset to transform the label values\n",
        "# Also remove the columns specified in 'remove_columns'\n",
        "\n",
        "dataset_base = dataset_base.map(transform_labels, remove_columns=remove_columns)"
      ],
      "metadata": {
        "id": "QHhZjOWaV3L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "yU-fnlomWDIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,            # Disable mixed-precision training\n",
        "    fp16_full_eval=False,  # Disable FP16 half precision evaluation\n",
        ")\n",
        "\n",
        "#use hub_model_id=\"finetuned-Sentiment-classfication-ROBERTA-model\n",
        "#use hub_model_id=\"finetuned-Sentiment-classfication-BERT-model\n",
        "#use hub_model_id=\"finetuned-Sentiment-classfication-DISTILBERT-model\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,                       # Number of epochs with no improvement before stopping training\n",
        "    early_stopping_threshold=0.01,                   # Minimum improvement in the metric for considering an improvement\n",
        ")\n",
        "\n",
        "# Combine the training arguments and the early stopping callback\n",
        "training_args.callbacks = [early_stopping]"
      ],
      "metadata": {
        "id": "iYznLLSaWHhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n"
      ],
      "metadata": {
        "id": "nHm76c3fWypv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_base = dataset_base['train'].shuffle(seed=10) #.select(range(40000)) # to select a part\n",
        "eval_dataset_base = dataset_base['eval'].shuffle(seed=10)\n"
      ],
      "metadata": {
        "id": "HeK6oHdRWtTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    rmse = np.sqrt(np.mean((predictions - labels)**2))\n",
        "    return {\"rmse\": rmse}"
      ],
      "metadata": {
        "id": "eeck5oldXBEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_base = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_base,\n",
        "    eval_dataset=eval_dataset_base,\n",
        "    compute_metrics=compute_metrics    # Add this line to define the compute_metrics function\n",
        ")"
      ],
      "metadata": {
        "id": "3-gqYKDlXDTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_base.train()"
      ],
      "metadata": {
        "id": "kdJgLEJ2XE6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}